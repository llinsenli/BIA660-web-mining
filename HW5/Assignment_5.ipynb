{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll need to use the following dataset:\n",
    "- text_train.json: This file contains a list of documents. It's used for training models\n",
    "- text_test.json: This file contains a list of documents and their ground-truth labels. It's used for testing performance. This file is in the format shown below. Note, each document has a list of labels.\n",
    "You can load these files using json.load()\n",
    "\n",
    "|Text| Labels|\n",
    "|----|-------|\n",
    "|paraglider collides with hot air balloon ... | ['Disaster and Accident', 'Travel & Transportation']|\n",
    "|faa issues fire warning for lithium ... | ['Travel & Transportation'] |\n",
    "| .... |...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: K-Mean Clustering\n",
    "\n",
    "Define a function **cluster_kmean()** as follows: \n",
    "- Take two file name strings as inputs: $train\\_file$ is the file path of text_train.json, and $test\\_file$ is the file path of text_test.json\n",
    "- When generating tfidf weights, set the min_df to 5.\n",
    "- Use **KMeans** to cluster documents in $train\\_file$ into 3 clusters by **cosine similarity**  and **Euclidean distance** separately. Use sufficient iterations with different initial centroids to make sure clustering converge \n",
    "- Test the clustering model performance using $test\\_file$: \n",
    "  * Predict the cluster ID for each document in $test\\_file$.\n",
    "  * Let's only use the **first label** in the ground-truth label list of each test document, e.g. for the first document in the table above, you set the ground_truth label to \"Disaster and Accident\" only.\n",
    "  * Apply **majority vote** rule to dynamically map the predicted cluster IDs to the ground-truth labels in $test\\_file$. **Be sure not to hardcode the mapping** (e.g. write code like {0: \"Disaster and Accident\"}), because a  cluster may corrspond to a different topic in each run. (hint: if you use pandas, look for \"idxmax\" function) \n",
    "  * Calculate **precision/recall/f-score** for each label, compare the results from the two clustering models, and write your analysis in a pdf file \n",
    "- This function has no return. Print out confusion matrix, precision/recall/f-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: LDA Clustering \n",
    "\n",
    "Q2.1. Define a function **cluster_lda()** as follows: \n",
    "1. Take two file name strings as inputs: $train\\_file$ is the file path of text_train.json, and $test\\_file$ is the file path of text_test.json\n",
    "2. Use **LDA** to train a topic model with documents in $train\\_file$ and the number of topics $K$ = 3. Keep min_df to 5 when generating tfidf weights, as in Q1.  \n",
    "3. Predict the topic distribution of each document in  $test\\_file$ and select the topic with highest probability. Similar to Q1, apply **majority vote rule** to map the topics to the labels and show the classification report. \n",
    "4. Return the array of topic proportion array\n",
    "\n",
    "Q2.2. Find similar documents\n",
    "- Define a function **find_similar_doc(doc_id, topic_mix)** to find **top 3 documents** that are the most similar to a selected one with index **doc_id** using the topic proportion array **topic_mix**. \n",
    "- You can calculate the cosine or Euclidean distance between two documents using the topic proportion array\n",
    "- Return the IDs of these similar documents.\n",
    "\n",
    "Q2.3. Provide a pdf document which contains: \n",
    "  - performance comparison between Q1 and Q2.1\n",
    "  - describe how you tune the model parameters, e.g. alpha, max_iter etc. in Q2.1.\n",
    "  - discuss how effective the method in Q2.2 is to find similar documents, compared with the tfidf weight cosine similarity we used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (Bonus): Biterm Topic Model (BTM)\n",
    "- There are many variants of LDA model. BTM is one designed for short text, while lDA in general expects documents with rich content.\n",
    "- Read this paper carefully http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&rep=rep1&type=pdf and try to understand the design\n",
    "- Try the following experiments:\n",
    "    - Script a few thousand tweets by different hastags\n",
    "    - Run LDA and BTM respectively to discover topics among the collected tweets. BTM package can be found at https://pypi.org/project/biterm/\n",
    "    - Compare the performance of each model. If one model works better, explain why it works better,\n",
    "- Summarize your experiment in a pdf document.\n",
    "- Note there is no absolute right or wrong answer in this experiment. All you need is to give a try and understand how BTM works and differences between BTM and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Due to randomness involved in these alogorithms, you may get the same result as what I showed below. However, your result should be close after you tune parameters carefully.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# addd your import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q1\n",
    "def cluster_kmean(train_file, test_file):\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q2\n",
    "def cluster_lda(train_file, test_file):\n",
    "    \n",
    "    topic_assign = None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return topic_assign\n",
    "\n",
    "def find_similar(doc_id, topic_assign):\n",
    "    \n",
    "    docs = None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1\n",
      "cosine\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                61                 2                      152\n",
      "1                               109                 7                       25\n",
      "2                                40               197                        7\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.77      0.52      0.62       210\n",
      "       News and Economy       0.81      0.96      0.88       206\n",
      "Travel & Transportation       0.71      0.83      0.76       184\n",
      "\n",
      "              micro avg       0.76      0.76      0.76       600\n",
      "              macro avg       0.76      0.77      0.75       600\n",
      "           weighted avg       0.76      0.76      0.75       600\n",
      "\n",
      "L2\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                               174                34                      174\n",
      "1                                31               166                       10\n",
      "2                                 5                 6                        0\n",
      "Cluster 0: Topic Disaster and Accident\n",
      "Cluster 1: Topic News and Economy\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.46      0.83      0.59       210\n",
      "       News and Economy       0.79      0.83      0.81       206\n",
      "Travel & Transportation       0.00      0.00      0.00       184\n",
      "\n",
      "              micro avg       0.58      0.58      0.58       600\n",
      "              macro avg       0.41      0.55      0.47       600\n",
      "           weighted avg       0.43      0.58      0.48       600\n",
      "\n",
      "\n",
      "Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rliu/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 25\n",
      "iteration: 2 of max_iter: 25\n",
      "iteration: 3 of max_iter: 25\n",
      "iteration: 4 of max_iter: 25\n",
      "iteration: 5 of max_iter: 25, perplexity: 3494.8408\n",
      "iteration: 6 of max_iter: 25\n",
      "iteration: 7 of max_iter: 25\n",
      "iteration: 8 of max_iter: 25\n",
      "iteration: 9 of max_iter: 25\n",
      "iteration: 10 of max_iter: 25, perplexity: 3416.5917\n",
      "iteration: 11 of max_iter: 25\n",
      "iteration: 12 of max_iter: 25\n",
      "iteration: 13 of max_iter: 25\n",
      "iteration: 14 of max_iter: 25\n",
      "iteration: 15 of max_iter: 25, perplexity: 3382.7160\n",
      "iteration: 16 of max_iter: 25\n",
      "iteration: 17 of max_iter: 25\n",
      "iteration: 18 of max_iter: 25\n",
      "iteration: 19 of max_iter: 25\n",
      "iteration: 20 of max_iter: 25, perplexity: 3377.7126\n",
      "iteration: 21 of max_iter: 25\n",
      "iteration: 22 of max_iter: 25\n",
      "iteration: 23 of max_iter: 25\n",
      "iteration: 24 of max_iter: 25\n",
      "iteration: 25 of max_iter: 25, perplexity: 3375.9923\n",
      "actual_class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                30                18                      138\n",
      "1                                12               182                        8\n",
      "2                               168                 6                       38\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic News and Economy\n",
      "Cluster 2: Topic Disaster and Accident\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.79      0.80      0.80       210\n",
      "       News and Economy       0.90      0.88      0.89       206\n",
      "Travel & Transportation       0.74      0.75      0.75       184\n",
      "\n",
      "              micro avg       0.81      0.81      0.81       600\n",
      "              macro avg       0.81      0.81      0.81       600\n",
      "           weighted avg       0.81      0.81      0.81       600\n",
      "\n",
      "cluster\n",
      "0    Travel & Transportation\n",
      "1           News and Economy\n",
      "2      Disaster and Accident\n",
      "dtype: object\n",
      "docs similar to 10: [337  38 222]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Due to randomness, you won't get the exact result\n",
    "    # as shown here, but your result should be close\n",
    "    # if you tune the parameters carefully\n",
    "    \n",
    "    # Q1\n",
    "    print(\"Q1\")\n",
    "    cluster_kmean('../../dataset/train_text.json', \\\n",
    "                  '../../dataset/test_text.json')\n",
    "            \n",
    "    # Q2\n",
    "    print(\"\\nQ2\")\n",
    "    topic_assign =cluster_lda('../../dataset/train_text.json', \\\n",
    "        '../../dataset/test_text.json')\n",
    "    doc_ids = find_similar(10, topic_assign)\n",
    "    print (\"docs similar to {0}: {1}\".format(10, doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 3 : Text Processing </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression\n",
    "Define a function \"**tokenize**\" as follows: \n",
    "   - takes a string as an input\n",
    "   - converts the string into lower case\n",
    "   - tokenizes the lower-cased string into tokens. A token is defined as follows:\n",
    "      - a token has at least 2 characters\n",
    "      - a token must start with an alphabetic letter (i.e. a-z or A-Z), \n",
    "      - a token can have alphabetic letters, \"\\-\" (hyphen), \".\" (dot), \"'\" (single quote), or \"\\_\" (underscore) in the middle\n",
    "      - a token must end with an alphabetic letter (i.e. a-z or A-Z) \n",
    "   - removes stop words from the tokens (use English stop words list from NLTK) \n",
    "   - returns the resulting token list as the output\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Sentimeent Analysis\n",
    "1. First define a function \"**sentiment_analysis**\" as follows: \n",
    "  - takes a string, a list of positive words, and a list of negative words as inputs. Assume the lists are read from positive-words.txt and negative-words.txt outside of this function.\n",
    "  - tokenizes the string using the tokeniz function defined above \n",
    "  - counts positive words and negative words in the tokens using the positive/negative words lists. With a list of negation words (i.e. not, no, isn't, wasn't, aren't, weren't, don't didn't, cannot, couldn't, won't, neither, nor), the final positive/negative words are defined as follows:\n",
    "    - Positive words:\n",
    "      * a positive word not preceded by a negation word \n",
    "      * a negative word preceded by a negation word\n",
    "    - Negative words:\n",
    "      * a negative word not preceded by a negation word\n",
    "      * a positive word preceded by a negation word\n",
    "    - determined the sentiment of the string as follows:\n",
    "       - 2: number of positive words > number of negative words\n",
    "       - 1: number of positive words <= number of negative words\n",
    "  - returns the sentiment\n",
    "    \n",
    "2. Define a function called **performance_evaluate** to evaluate the accuracy of the sentiment analysis in (1) as follows:  \n",
    "   - takes an input file (\"amazon_review_300.csv\"), a list of positive words, and a list of negative words as inputs. The input file has a list of reviews in the format of (label, review).\n",
    "   - reads the input file to get a list of reviews including review text and label of each review \n",
    "   - for each review, predicts its sentiment using the function defined in \n",
    "   - returns the accuracy as the number of correct sentiment predictions/total reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: (Bonus) Vector Space Model\n",
    "\n",
    "1. Define a function **find_similar_doc** as follows: \n",
    "    - takes two inputs: a list of documents (i.e. docs), and the index of a selected document as an integer (i.e. doc_id).\n",
    "    - uses the \"tokenize\" function defined in Q1 to tokenize each document \n",
    "    - generates normalized tf_idf matrix (each row is normalized) from the tokens (hint: reference to the tf_idf function defined in Section 8.5 in lecture notes) \n",
    "    - calculates the pairwise cosine distance of documents using the generated tf_idf matrix \n",
    "    - for the selected doc_id, finds the index of the most similar document (but not itself) by the cosine similarity score \n",
    "    - returns the index of the most similar document and the similarity score\n",
    "2. Test your function with \"amazon_review_300.csv\" and a few reviews from this file.\n",
    "   - Check the most similar review discovered for each of the selected reviews\n",
    "   - Can you use the calculated similarity score to determine if two documents are similar?  \n",
    "   - Do you think this function can successfully find similar documents? Why does it work or not work? \n",
    "   - If it does not work, what can you do to improve the search?\n",
    "   - Write down your analysis along with some evidence or observations you have in a pdf file and submit this pdf file along with your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    tokens = None\n",
    "    \n",
    "    # add your code\n",
    "    \n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(text, positive_words, negative_words):\n",
    "    \n",
    "    negations=[\"not\", \"no\", \"isn't\", \"wasn't\", \"aren't\", \\\n",
    "               \"weren't\", \"don't\", \"didn't\", \"cannot\", \\\n",
    "               \"couldn't\", \"won't\", \"neither\", \"nor\"]\n",
    "    \n",
    "    sentiment = None\n",
    "    \n",
    "    # add your code\n",
    "            \n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def performance_evaluate(input_file, positive_words, negative_words):\n",
    "    \n",
    "    accuracy = None\n",
    "    \n",
    "    # add your code\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q3\n",
    "\n",
    "def find_similar_doc(docs,  doc_id):\n",
    "    \n",
    "    top_sim_index, top_sim_score = None, None\n",
    "    \n",
    "    # add your code\n",
    "    \n",
    "    return top_sim_index, top_sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 tokens: ['composed', 'cds', 'quite', 'songs', 'exact', 'count', 'heart-rendering', 'impressively', 'remarkable', 'everything', 'every', 'listener', 'fast-paced', 'energetic', 'dancing', 'tokage', 'termina', 'home', 'slower', 'haunting', 'dragon', 'god', 'purely', 'beautifully', 'composed', \"time's\", 'scar', 'even', 'fantastic', 'vocals', 'radical', 'dreamers', 'one', 'best', 'videogame', 'soundtracks', 'surely', \"mitsuda's\", 'best', 'ever']\n",
      "\n",
      "Q2 accuracy: 0.71\n",
      "\n",
      "Similarity between 207 and 206 is 0.20: \n",
      "\n",
      "selected doc:  I have been using this product for 5+ years. It was wonderful. About 1 year ago the company changed packaging and the product changed slightly. The bottle is taller now and something is missing from the serum. Doesn't work as well as it used to work. I will not be purchasing this item because of the change.\n",
      "\n",
      "similar doc:  I have been used this product for many years. But somehow the product I received this time is like fake one. It's very thin. I have to used double amount.\n",
      "\n",
      "Similarity between 10 and 15 is 0.16: \n",
      "\n",
      "selected doc:  A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\n",
      "\n",
      "similar doc:  It's glaringly obvious that all of the glowing reviews have been written by the same person, perhaps the author herself. They all have the same misspellings and poor sentence structure that is featured in the book. Who made Veronica Haddon think she is an author?\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Q1\n",
    "    text=\"Composed of 3 CDs and quite a few songs (I haven't an exact count), \\\n",
    "          all of which are heart-rendering and impressively remarkable. \\\n",
    "          It has everything for every listener -- from fast-paced and energetic \\\n",
    "          (Dancing the Tokage or Termina Home), to slower and more haunting (Dragon God), \\\n",
    "          to purely beautifully composed (Time's Scar), \\\n",
    "          to even some fantastic vocals (Radical Dreamers).\\\n",
    "          This is one of the best videogame soundtracks out there, \\\n",
    "          and surely Mitsuda's best ever. ^_^\"\n",
    "\n",
    "    tokens=tokenize(text)\n",
    "    \n",
    "    print(\"Q1 tokens:\", tokens)\n",
    "    \n",
    "    # Test Q2\n",
    "    \n",
    "    with open(\"../../dataset/positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"../../dataset/negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        \n",
    "    acc=performance_evaluate(\"../../dataset/amazon_review_300.csv\", \\\n",
    "                                  positive_words, negative_words)\n",
    "    print(\"\\nQ2 accuracy: {0:.2f}\".format(acc))\n",
    "   \n",
    "\n",
    "    # Test Q3\n",
    "    data = pd.read_csv(\"../../dataset/amazon_review_300.csv\")\n",
    "    # pick any doc id, e.g. 10, 207\n",
    "    doc_id =207\n",
    "    sim_doc_id, sim = find_similar_doc(data[\"review\"], doc_id)\n",
    "    print(\"\\nSimilarity between {0} and {1} is {2:.2f}: \"\\\n",
    "          .format(doc_id, sim_doc_id, sim))\n",
    "    print(\"\\nselected doc: \", data.loc[doc_id][\"review\"])\n",
    "    print(\"\\nsimilar doc: \", data.loc[sim_doc_id][\"review\"])\n",
    "    \n",
    "    \n",
    "    doc_id =10\n",
    "    sim_doc_id, sim = find_similar_doc(data[\"review\"], doc_id)\n",
    "    print(\"\\nSimilarity between {0} and {1} is {2:.2f}: \"\\\n",
    "          .format(doc_id, sim_doc_id, sim))\n",
    "    print(\"\\nselected doc: \", data.loc[doc_id][\"review\"])\n",
    "    print(\"\\nsimilar doc: \", data.loc[sim_doc_id][\"review\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
